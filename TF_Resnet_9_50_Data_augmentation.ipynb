{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Resnet 9/50_Data_augmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLmPDNyYKFzRqmmQ6pkFgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbanai/Multi-label-classification_DOLPHIN_HORSE_Part1_Pytorch-Conv2DandResnet9_Tensorflow_DNNandCNN/blob/main/TF_Resnet_9_50_Data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boNNoWXjLNYB"
      },
      "source": [
        "#------Experimenting with Tensorflow CNN model with DATAAUGMENTATION ------\n",
        "#------ with dataaugmentation I reached worse result with TF than without it\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqzpKrhE17aM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa6082c-e4cd-47a8-c836-7beed67e807c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AbHqSqjlJVv"
      },
      "source": [
        "path=\".\"\n",
        "os.chdir(path)\n",
        "os.makedirs(\"train\")\n",
        "os.makedirs(\"valid\")\n",
        "os.makedirs(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSiLgwFSk10u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4445ba-057b-4b1d-fd2a-3139e49a3686"
      },
      "source": [
        "!pip install pyunpack\n",
        "!pip install patool\n",
        "from pyunpack import Archive\n",
        "Archive('/content/drive/MyDrive/dolphin_train.rar').extractall('/content/train/')\n",
        "Archive('/content/drive/MyDrive/horse_train.rar').extractall('/content/train/')\n",
        "Archive(\"/content/drive/MyDrive/dolphin_horse_train.rar\").extractall('/content/train/')\n",
        "Archive('/content/drive/MyDrive/horse_valid.rar').extractall('/content/valid/')\n",
        "Archive('/content/drive/MyDrive/dolphin_valid.rar').extractall('/content/valid/')\n",
        "Archive('/content/drive/MyDrive/dolphin_horse_valid.rar').extractall('/content/valid/')\n",
        "Archive('/content/drive/MyDrive/DolphinTest.rar').extractall('/content/test/')\n",
        "Archive('/content/drive/MyDrive/HorseTest.rar').extractall('/content/test/')\n",
        "Archive(\"/content/drive/MyDrive/DolphinHorseTest.rar\").extractall('/content/test/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyunpack\n",
            "  Downloading https://files.pythonhosted.org/packages/83/29/020436b1d8e96e5f26fa282b9c3c13a3b456a36b9ea2edc87c5fed008369/pyunpack-0.2.2-py2.py3-none-any.whl\n",
            "Collecting entrypoint2\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/b0/8ef4b1d8be02448d164c52466530059d7f57218655d21309a0c4236d7454/entrypoint2-0.2.4-py3-none-any.whl\n",
            "Collecting easyprocess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: entrypoint2, easyprocess, pyunpack\n",
            "Successfully installed easyprocess-0.3 entrypoint2-0.2.4 pyunpack-0.2.2\n",
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWg9Fui6xEzz"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEMdqf0KpFmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "00ce69ba-e78c-4021-abc6-1349a28c2687"
      },
      "source": [
        "gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "                                                         #rotation_range=20,\n",
        "                                                         #horizontal_flip=True,\n",
        "                                                         #width_shift_range=0.2, \n",
        "                                                         #height_shift_range=0.2,\n",
        "                                                         #shear_range=0.2, \n",
        "                                                         #zoom_range=0.2)\n",
        "\n",
        "\n",
        "data_augmentation=keras.Sequential([ \n",
        "                        layers.experimental.preprocessing.RandomContrast(factor=0.9),\n",
        "                        layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"),\n",
        "                        layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "                        #layers.experimental.preprocessing.RandomCrop(30,30),\n",
        "                        layers.experimental.preprocessing.RandomZoom(0.3)\n",
        "                      \n",
        "                             \n",
        "\n",
        "])\n",
        "\n",
        "# Checking how to create more image from 1\n",
        "\n",
        "image_path=\"/content/test/DolphinTest/aagewag.jpg\"\n",
        "#image= cv2.imread(image_path)\n",
        "image2 = np.expand_dims(cv2.imread(image_path), axis=0)\n",
        "aug_image=gen.flow(image2)\n",
        "aug_images=[next(aug_image)[0] for i in range(10)]\n",
        "\n",
        "def show2(batch):    \n",
        "    for i in range(10):\n",
        "        plt.subplot(2,5,i+1)\n",
        "        plt.imshow(batch[i])\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        \n",
        "        \n",
        "show2(aug_images)\n",
        "\n",
        "#plt.imshow(next(aug_image)[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_RlAKC0pe5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cceff42-8ecd-49f8-a3b7-610787045ab8"
      },
      "source": [
        "image_path=\"/content/test/DolphinTest/aagewag.jpg\"\n",
        "#image= cv2.imread(image_path)\n",
        "image2 = np.expand_dims(cv2.imread(image_path), axis=0)\n",
        "aug_image=data_augmentation(image2)\n",
        "\n",
        "print(type(aug_image))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LbiR2GNG9Y6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c0c5af-f1d7-4edb-d8c2-ae742e2f3c83"
      },
      "source": [
        "#--------------Creating TRAINING, VALIDATION AND TEST NP ARRAY FOR THE CNN MODEL------\n",
        "\n",
        "datadir=\"/content/train/\" \n",
        "train_categories=['horse_train/', 'dolphin_train/', 'dolphin_horse_train/']\n",
        "\n",
        "dataset_train=[]\n",
        "def create_train_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    if category==\"horse_train/\":\n",
        "      class_num_train=[1,0]\n",
        "    if category==\"dolphin_train/\":\n",
        "      class_num_train=[0,1]\n",
        "    if category==\"dolphin_horse_train/\":\n",
        "      class_num_train=[1,1]\n",
        "    for img in os.listdir(path_train):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (64, 64)) \n",
        "        new_array1=new_array/255 \n",
        "        dataset_train.append([new_array1, class_num_train]) \n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_train.append([next(aug_image)[0], class_num_train])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_train_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_train)\n",
        "\n",
        "inputs_train=[]\n",
        "targets_train=[]\n",
        "\n",
        "for image, label in dataset_train:\n",
        "  inputs_train.append(image)\n",
        "  targets_train.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_train = np.array(inputs_train)\n",
        "arr_inputs_float_train=arr_inputs_train.astype('float32')\n",
        "\n",
        "\n",
        "arr_inputs_train_targets = np.array(targets_train)\n",
        "arr_inputs_float_train_targets=arr_inputs_train_targets.astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datadir=\"/content/valid/\" \n",
        "valid_categories=['horse_valid/', 'dolphin_valid/', 'dolphin_horse_valid/']\n",
        "\n",
        "dataset_valid=[]\n",
        "def create_validation_data():\n",
        "  for category in valid_categories:\n",
        "    path_valid=os.path.join(datadir, category)\n",
        "    if category==\"horse_valid/\":\n",
        "      class_num_valid=[1,0]\n",
        "    if category==\"dolphin_valid/\":\n",
        "      class_num_valid=[0,1]\n",
        "    if category==\"dolphin_horse_valid/\":\n",
        "      class_num_valid=[1,1]\n",
        "    for img in os.listdir(path_valid):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_valid, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (64, 64))\n",
        "        new_array1=new_array/255 \n",
        "        dataset_valid.append([new_array1, class_num_valid])  \n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_valid.append([next(aug_image)[0], class_num_valid])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_validation_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_valid)\n",
        "\n",
        "inputs_val=[]\n",
        "targets_val=[]\n",
        "\n",
        "for image, label in dataset_valid:\n",
        "  inputs_val.append(image)\n",
        "  targets_val.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_val = np.array(inputs_val)\n",
        "arr_inputs_float_val=arr_inputs_val.astype('float32')\n",
        "\n",
        "arr_inputs_val_targets = np.array(targets_val)\n",
        "arr_inputs_float_val_targets=arr_inputs_val_targets.astype('float32')\n",
        "\n",
        "\n",
        "datadir=\"/content/test/\" \n",
        "test_categories=['HorseTest/', 'DolphinTest/', 'DolphinHorseTest/']\n",
        "\n",
        "dataset_test=[]\n",
        "def create_test_data():\n",
        "  for category in test_categories:\n",
        "    path_test=os.path.join(datadir, category)\n",
        "    if category==\"HorseTest/\":\n",
        "      class_num_test=[1,0]\n",
        "    if category==\"DolphinTest/\":\n",
        "      class_num_test=[0,1]\n",
        "    if category==\"DolphinHorseTest/\":\n",
        "      class_num_test=[1,1]\n",
        "    for img in os.listdir(path_test):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_test, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (64, 64)) \n",
        "        new_array1=new_array/255 \n",
        "        dataset_test.append([new_array1, class_num_test])\n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_test.append([next(aug_image)[0], class_num_test])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_test_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_test)\n",
        "\n",
        "inputs_test=[]\n",
        "targets_test=[]\n",
        "\n",
        "for image, label in dataset_test:\n",
        "  inputs_test.append(image)\n",
        "  targets_test.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_test = np.array(inputs_test)\n",
        "arr_inputs_float_test=arr_inputs_test.astype('float32')\n",
        "\n",
        "arr_inputs_test_targets = np.array(targets_test)\n",
        "arr_inputs_float_test_targets=arr_inputs_test_targets.astype('float32')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUW9F-mSKKFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07da2f58-634f-4af7-a5e9-613b367a0cdd"
      },
      "source": [
        "modelCNN = keras.models.Sequential()\n",
        "modelCNN.add(data_augmentation)\n",
        "modelCNN.add(layers.Conv2D(32, (3,3), strides=(1,1), padding=\"valid\", activation='relu', input_shape=(64,64,3)))  #64\n",
        "modelCNN.add(layers.MaxPool2D((2,2)))   #32\n",
        "modelCNN.add(layers.Conv2D(64, 3, activation='relu'))\n",
        "modelCNN.add(layers.MaxPool2D((2,2)))  #16\n",
        "modelCNN.add(layers.Conv2D(128, 3, activation='relu'))\n",
        "modelCNN.add(layers.MaxPool2D((2,2)))  #8\n",
        "modelCNN.add(layers.Conv2D(256, 3, activation='relu'))\n",
        "modelCNN.add(layers.MaxPool2D((2,2)))  #4\n",
        "modelCNN.add(layers.MaxPool2D((2,2)))  #2\n",
        "modelCNN.add(layers.Flatten())\n",
        "modelCNN.add(layers.Dense(1024, activation='relu'))\n",
        "modelCNN.add(layers.Dropout(0.2))\n",
        "modelCNN.add(layers.Dense(512, activation='relu'))\n",
        "modelCNN.add(layers.Dropout(0.2))\n",
        "modelCNN.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# 2.) lOSS, OPTIMIZER, METRICS\n",
        "loss = keras.losses.BinaryCrossentropy(from_logits=False)  #SparseCategoricalCrossentropy as we have single classes(1,2), \n",
        "                                                                    #if it is vector (0,1,0,0....) CategoricalCrossentropy. \n",
        "                                                                    #From_logits=True as we didn't build the softmax into the model\n",
        "optim = keras.optimizers.Adam(lr=0.001)\n",
        "metrics = [\"accuracy\"]\n",
        "\n",
        "# 3.) CONFIGURATION OF THE MODEL\n",
        "\n",
        "modelCNN.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "\n",
        "# 4.) TRAINING\n",
        "batch_size = 32\n",
        "epochs = 40\n",
        "\n",
        "modelCNN.fit(arr_inputs_float_train, arr_inputs_float_train_targets,\n",
        "             validation_data=(arr_inputs_float_val, arr_inputs_float_val_targets), \n",
        "             batch_size=batch_size, epochs=epochs, shuffle=True, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "15/15 - 31s - loss: 0.6388 - accuracy: 0.6385 - val_loss: 0.6933 - val_accuracy: 0.7822\n",
            "Epoch 2/40\n",
            "15/15 - 0s - loss: 0.5508 - accuracy: 0.7468 - val_loss: 0.3409 - val_accuracy: 0.8911\n",
            "Epoch 3/40\n",
            "15/15 - 0s - loss: 0.4384 - accuracy: 0.7944 - val_loss: 0.3022 - val_accuracy: 0.8713\n",
            "Epoch 4/40\n",
            "15/15 - 0s - loss: 0.4121 - accuracy: 0.8203 - val_loss: 0.2589 - val_accuracy: 0.8614\n",
            "Epoch 5/40\n",
            "15/15 - 0s - loss: 0.3975 - accuracy: 0.7987 - val_loss: 0.2534 - val_accuracy: 0.8119\n",
            "Epoch 6/40\n",
            "15/15 - 0s - loss: 0.4043 - accuracy: 0.7900 - val_loss: 0.3813 - val_accuracy: 0.8614\n",
            "Epoch 7/40\n",
            "15/15 - 0s - loss: 0.5301 - accuracy: 0.7532 - val_loss: 0.4265 - val_accuracy: 0.8416\n",
            "Epoch 8/40\n",
            "15/15 - 0s - loss: 0.4399 - accuracy: 0.8117 - val_loss: 0.2537 - val_accuracy: 0.7822\n",
            "Epoch 9/40\n",
            "15/15 - 0s - loss: 0.3569 - accuracy: 0.8030 - val_loss: 0.2131 - val_accuracy: 0.8614\n",
            "Epoch 10/40\n",
            "15/15 - 0s - loss: 0.3389 - accuracy: 0.8139 - val_loss: 0.2238 - val_accuracy: 0.8812\n",
            "Epoch 11/40\n",
            "15/15 - 0s - loss: 0.3448 - accuracy: 0.8160 - val_loss: 0.2349 - val_accuracy: 0.8218\n",
            "Epoch 12/40\n",
            "15/15 - 0s - loss: 0.3591 - accuracy: 0.7944 - val_loss: 0.2151 - val_accuracy: 0.8713\n",
            "Epoch 13/40\n",
            "15/15 - 0s - loss: 0.3608 - accuracy: 0.8290 - val_loss: 0.1885 - val_accuracy: 0.8713\n",
            "Epoch 14/40\n",
            "15/15 - 0s - loss: 0.3287 - accuracy: 0.8225 - val_loss: 0.2241 - val_accuracy: 0.9010\n",
            "Epoch 15/40\n",
            "15/15 - 0s - loss: 0.3432 - accuracy: 0.8203 - val_loss: 0.2235 - val_accuracy: 0.8812\n",
            "Epoch 16/40\n",
            "15/15 - 0s - loss: 0.3586 - accuracy: 0.8030 - val_loss: 0.2400 - val_accuracy: 0.8713\n",
            "Epoch 17/40\n",
            "15/15 - 0s - loss: 0.3183 - accuracy: 0.8074 - val_loss: 0.2352 - val_accuracy: 0.8614\n",
            "Epoch 18/40\n",
            "15/15 - 0s - loss: 0.3404 - accuracy: 0.8139 - val_loss: 0.2670 - val_accuracy: 0.7822\n",
            "Epoch 19/40\n",
            "15/15 - 0s - loss: 0.3227 - accuracy: 0.8355 - val_loss: 0.2243 - val_accuracy: 0.8416\n",
            "Epoch 20/40\n",
            "15/15 - 0s - loss: 0.3071 - accuracy: 0.8139 - val_loss: 0.2139 - val_accuracy: 0.8515\n",
            "Epoch 21/40\n",
            "15/15 - 0s - loss: 0.3350 - accuracy: 0.8139 - val_loss: 0.2401 - val_accuracy: 0.8614\n",
            "Epoch 22/40\n",
            "15/15 - 0s - loss: 0.3616 - accuracy: 0.8268 - val_loss: 0.2160 - val_accuracy: 0.8713\n",
            "Epoch 23/40\n",
            "15/15 - 0s - loss: 0.3157 - accuracy: 0.8312 - val_loss: 0.1986 - val_accuracy: 0.8515\n",
            "Epoch 24/40\n",
            "15/15 - 0s - loss: 0.3156 - accuracy: 0.8355 - val_loss: 0.2099 - val_accuracy: 0.8416\n",
            "Epoch 25/40\n",
            "15/15 - 0s - loss: 0.3180 - accuracy: 0.8442 - val_loss: 0.2157 - val_accuracy: 0.8317\n",
            "Epoch 26/40\n",
            "15/15 - 0s - loss: 0.3194 - accuracy: 0.8420 - val_loss: 0.2489 - val_accuracy: 0.7921\n",
            "Epoch 27/40\n",
            "15/15 - 0s - loss: 0.2955 - accuracy: 0.8398 - val_loss: 0.2912 - val_accuracy: 0.7525\n",
            "Epoch 28/40\n",
            "15/15 - 0s - loss: 0.3133 - accuracy: 0.8398 - val_loss: 0.2398 - val_accuracy: 0.8020\n",
            "Epoch 29/40\n",
            "15/15 - 0s - loss: 0.3086 - accuracy: 0.8398 - val_loss: 0.2014 - val_accuracy: 0.8713\n",
            "Epoch 30/40\n",
            "15/15 - 0s - loss: 0.2992 - accuracy: 0.8268 - val_loss: 0.2126 - val_accuracy: 0.8317\n",
            "Epoch 31/40\n",
            "15/15 - 0s - loss: 0.3105 - accuracy: 0.8355 - val_loss: 0.2666 - val_accuracy: 0.7723\n",
            "Epoch 32/40\n",
            "15/15 - 0s - loss: 0.3302 - accuracy: 0.8290 - val_loss: 0.3347 - val_accuracy: 0.7624\n",
            "Epoch 33/40\n",
            "15/15 - 0s - loss: 0.3591 - accuracy: 0.8442 - val_loss: 0.2219 - val_accuracy: 0.8416\n",
            "Epoch 34/40\n",
            "15/15 - 0s - loss: 0.3301 - accuracy: 0.8377 - val_loss: 0.2057 - val_accuracy: 0.8416\n",
            "Epoch 35/40\n",
            "15/15 - 0s - loss: 0.2844 - accuracy: 0.8377 - val_loss: 0.1741 - val_accuracy: 0.9010\n",
            "Epoch 36/40\n",
            "15/15 - 0s - loss: 0.2816 - accuracy: 0.8550 - val_loss: 0.1989 - val_accuracy: 0.8416\n",
            "Epoch 37/40\n",
            "15/15 - 0s - loss: 0.2834 - accuracy: 0.8377 - val_loss: 0.1678 - val_accuracy: 0.8416\n",
            "Epoch 38/40\n",
            "15/15 - 0s - loss: 0.2983 - accuracy: 0.8442 - val_loss: 0.2650 - val_accuracy: 0.7921\n",
            "Epoch 39/40\n",
            "15/15 - 0s - loss: 0.2984 - accuracy: 0.8398 - val_loss: 0.1824 - val_accuracy: 0.8317\n",
            "Epoch 40/40\n",
            "15/15 - 0s - loss: 0.3175 - accuracy: 0.8442 - val_loss: 0.2093 - val_accuracy: 0.8218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78445dc810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAN5wPBbhOZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f17f462-3c54-4ab8-90a2-2eab7efb4c83"
      },
      "source": [
        "\n",
        "modelCNN.evaluate(arr_inputs_float_test, arr_inputs_float_test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2224 - accuracy: 0.7973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22243528068065643, 0.7972972989082336]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0cHZ08xk8f4"
      },
      "source": [
        "Experiment with resnet 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbNWoZmMk7oj"
      },
      "source": [
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "#from keras.utils import plot_model\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMABvyNMrPlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68efe166-0911-4017-b1bf-4cf189b7ba16"
      },
      "source": [
        "datadir=\"/content/train/\" \n",
        "train_categories=['horse_train/', 'dolphin_train/', 'dolphin_horse_train/']\n",
        "\n",
        "dataset_train=[]\n",
        "def create_train_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    if category==\"horse_train/\":\n",
        "      class_num_train=[1,0]\n",
        "    if category==\"dolphin_train/\":\n",
        "      class_num_train=[0,1]\n",
        "    if category==\"dolphin_horse_train/\":\n",
        "      class_num_train=[1,1]\n",
        "    for img in os.listdir(path_train):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (224, 224)) \n",
        "        new_array1=new_array/255 \n",
        "        #scaling=StandardScaler()\n",
        "        #new_array1=scaling.fit_transform(new_array.reshape(-1, new_array.shape[-1])).reshape(new_array)\n",
        "        dataset_train.append([new_array1, class_num_train]) \n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_train.append([next(aug_image)[0], class_num_train])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_train_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_train)\n",
        "\n",
        "inputs_train=[]\n",
        "targets_train=[]\n",
        "\n",
        "for image, label in dataset_train:\n",
        "  inputs_train.append(image)\n",
        "  targets_train.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_train = np.array(inputs_train)\n",
        "arr_inputs_float_train=arr_inputs_train.astype('float32')\n",
        "\n",
        "\n",
        "arr_inputs_train_targets = np.array(targets_train)\n",
        "arr_inputs_float_train_targets=arr_inputs_train_targets.astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datadir=\"/content/valid/\" \n",
        "valid_categories=['horse_valid/', 'dolphin_valid/', 'dolphin_horse_valid/']\n",
        "\n",
        "dataset_valid=[]\n",
        "def create_validation_data():\n",
        "  for category in valid_categories:\n",
        "    path_valid=os.path.join(datadir, category)\n",
        "    if category==\"horse_valid/\":\n",
        "      class_num_valid=[1,0]\n",
        "    if category==\"dolphin_valid/\":\n",
        "      class_num_valid=[0,1]\n",
        "    if category==\"dolphin_horse_valid/\":\n",
        "      class_num_valid=[1,1]\n",
        "    for img in os.listdir(path_valid):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_valid, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (224, 224))\n",
        "        new_array1=new_array/255 \n",
        "        #scaling=StandardScaler()\n",
        "        #new_array1=scaling.fit_transform(new_array.reshape(-1, new_array.shape[-1])).reshape(new_array)\n",
        "        dataset_valid.append([new_array1, class_num_valid])  \n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_valid.append([next(aug_image)[0], class_num_valid])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_validation_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_valid)\n",
        "\n",
        "inputs_val=[]\n",
        "targets_val=[]\n",
        "\n",
        "for image, label in dataset_valid:\n",
        "  inputs_val.append(image)\n",
        "  targets_val.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_val = np.array(inputs_val)\n",
        "arr_inputs_float_val=arr_inputs_val.astype('float32')\n",
        "\n",
        "arr_inputs_val_targets = np.array(targets_val)\n",
        "arr_inputs_float_val_targets=arr_inputs_val_targets.astype('float32')\n",
        "\n",
        "\n",
        "datadir=\"/content/test/\" \n",
        "test_categories=['HorseTest/', 'DolphinTest/', 'DolphinHorseTest/']\n",
        "\n",
        "dataset_test=[]\n",
        "def create_test_data():\n",
        "  for category in test_categories:\n",
        "    path_test=os.path.join(datadir, category)\n",
        "    if category==\"HorseTest/\":\n",
        "      class_num_test=[1,0]\n",
        "    if category==\"DolphinTest/\":\n",
        "      class_num_test=[0,1]\n",
        "    if category==\"DolphinHorseTest/\":\n",
        "      class_num_test=[1,1]\n",
        "    for img in os.listdir(path_test):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_test, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (224, 224)) \n",
        "        new_array1=new_array/255 \n",
        "        #scaling=StandardScaler()\n",
        "        #scalers = {}\n",
        "        #new_array1=scaling.fit_transform(new_array.reshape(-1, new_array.shape[-1])).reshape(new_array)\n",
        "        dataset_test.append([new_array1, class_num_test])\n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_test.append([next(aug_image)[0], class_num_test])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_test_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_test)\n",
        "\n",
        "inputs_test=[]\n",
        "targets_test=[]\n",
        "\n",
        "for image, label in dataset_test:\n",
        "  inputs_test.append(image)\n",
        "  targets_test.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_test = np.array(inputs_test)\n",
        "arr_inputs_float_test=arr_inputs_test.astype('float32')\n",
        "\n",
        "arr_inputs_test_targets = np.array(targets_test)\n",
        "arr_inputs_float_test_targets=arr_inputs_test_targets.astype('float32')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_RqIRs0mhmS"
      },
      "source": [
        "def identity_block(X, f, filters):\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters  # F1=64,F2=64,256\n",
        "    \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First  layer\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    \n",
        "    # Second  layer\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third  layer\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "    # Final step: Add shortcut value to F(X), and pass it through a RELU activation \n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCV8syXQm1TA"
      },
      "source": [
        "def convolutional_block(X, f, filters, s = 2):\n",
        "\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    # First layer \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s))(X) # 1,1 is filter size\n",
        "    X = BatchNormalization(axis = 3)(X)  # normalization on channels\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "      \n",
        "    # Second layer  (f,f)=3*3 filter by default\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "    # Third layer\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "\n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid')(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value here, and pass it through a RELU activation \n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veX7BjnanAtA"
      },
      "source": [
        "def ResNet50(input_shape=(224, 224, 3), classes=2):\n",
        "    \"\"\"\n",
        "    Implementation of the ResNet50 architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input) #3,3 padding\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2))(X) \n",
        "    X = BatchNormalization(axis=3)(X) \n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], s=2)\n",
        " \n",
        "   \n",
        "    # below 3 lines are the conv layers from convolutional_block function defined above\n",
        "    #X = Conv2D(F1, (1, 1), strides = (s,s))(X)\n",
        "    #X = Conv2D(F2, kernel_size = (f, f), strides = (1,1), padding = 'same')(X)\n",
        "    #X = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '2a')(X)\n",
        "   \n",
        "    X = identity_block(X, 3, [64, 64, 256]) \n",
        "    #X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "    #X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same')(X)\n",
        "    #X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "  \n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    #X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "    #X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same')(X)\n",
        "    #X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n",
        "\n",
        "\n",
        "    # Stage 3 \n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 1)\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "\n",
        "    # Stage 4 \n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    # Stage 5 \n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    # AVGPOOL \n",
        "    X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X=tf.keras.layers.Dropout(0.2)(X)\n",
        "    #X=Dropout(0.2)(X)\n",
        "    X=Dense(1024, activation='relu')(X)\n",
        "    X=Dense(512, activation='relu')(X)\n",
        "    X = Dense(classes, activation='sigmoid', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfBC2Y1u1-yx"
      },
      "source": [
        "model = ResNet50(input_shape = (224, 224, 3), classes = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29r6kks99_c"
      },
      "source": [
        "#trying lr scheduler\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optim = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(optimizer=optim, loss='BinaryCrossentropy', metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(arr_inputs_float_train, arr_inputs_float_train_targets,\n",
        "          validation_data=(arr_inputs_float_val, arr_inputs_float_val_targets),\n",
        "          epochs = 100, callbacks=[lr_schedule], batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "GlZSCLIw_RtG",
        "outputId": "0540309a-ca18-48b3-eeb4-34ef628e3de3"
      },
      "source": [
        "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
        "plt.plot(lrs, history.history[\"loss\"])\n",
        "plt.semilogx(lrs, history.history[\"loss\"])\n",
        "#plt.axis([1e-8, 1e-3, 0, 300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f94f21a2fd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRnAsX6u_HAo",
        "outputId": "6d9409e3-9631-4e01-ec98-d895526cb81a"
      },
      "source": [
        "a=history.history[\"loss\"]\n",
        "b=a.index(min(a))\n",
        "print(lrs[b])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0002511886431509582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLhjIIxBnQm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31dd567b-22ed-4376-eff0-623ef4dc94d3"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "optim = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optim, loss='BinaryCrossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(arr_inputs_float_train, arr_inputs_float_train_targets,\n",
        "          validation_data=(arr_inputs_float_val, arr_inputs_float_val_targets),\n",
        "          epochs = 100, batch_size = 32)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 28s 912ms/step - loss: 2.2319 - accuracy: 0.6255 - val_loss: 3.7916 - val_accuracy: 0.5941\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.8202 - accuracy: 0.7084 - val_loss: 23.8373 - val_accuracy: 0.5941\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 9s 583ms/step - loss: 0.4685 - accuracy: 0.7961 - val_loss: 878.1693 - val_accuracy: 0.6040\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 9s 583ms/step - loss: 0.4013 - accuracy: 0.8223 - val_loss: 728.3929 - val_accuracy: 0.5941\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.3659 - accuracy: 0.8487 - val_loss: 295.8806 - val_accuracy: 0.4851\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.3880 - accuracy: 0.8468 - val_loss: 1724.4237 - val_accuracy: 0.4356\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.4319 - accuracy: 0.7935 - val_loss: 1794.2845 - val_accuracy: 0.7228\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.3679 - accuracy: 0.7683 - val_loss: 494.9453 - val_accuracy: 0.7921\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.4880 - accuracy: 0.8500 - val_loss: 4.1360 - val_accuracy: 0.8614\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.3171 - accuracy: 0.8810 - val_loss: 30.8651 - val_accuracy: 0.8119\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.3182 - accuracy: 0.8638 - val_loss: 57.4899 - val_accuracy: 0.7822\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.4370 - accuracy: 0.8359 - val_loss: 1.0726 - val_accuracy: 0.7525\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.2841 - accuracy: 0.8588 - val_loss: 0.9971 - val_accuracy: 0.8218\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.2564 - accuracy: 0.8752 - val_loss: 0.3850 - val_accuracy: 0.8416\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.2732 - accuracy: 0.8628 - val_loss: 1.4857 - val_accuracy: 0.6832\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.2325 - accuracy: 0.8874 - val_loss: 0.6891 - val_accuracy: 0.7426\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.2136 - accuracy: 0.8875 - val_loss: 0.3894 - val_accuracy: 0.8515\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.2101 - accuracy: 0.8581 - val_loss: 0.3911 - val_accuracy: 0.8911\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.1536 - accuracy: 0.9213 - val_loss: 0.6718 - val_accuracy: 0.8317\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1679 - accuracy: 0.8808 - val_loss: 0.2996 - val_accuracy: 0.9010\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.1792 - accuracy: 0.8631 - val_loss: 0.2119 - val_accuracy: 0.9010\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.1898 - accuracy: 0.8889 - val_loss: 0.1625 - val_accuracy: 0.9208\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1383 - accuracy: 0.9519 - val_loss: 0.9164 - val_accuracy: 0.8614\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.2104 - accuracy: 0.8886 - val_loss: 0.9302 - val_accuracy: 0.6634\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1747 - accuracy: 0.9040 - val_loss: 1.3196 - val_accuracy: 0.6040\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1631 - accuracy: 0.9069 - val_loss: 1.0293 - val_accuracy: 0.6436\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1434 - accuracy: 0.9115 - val_loss: 0.8062 - val_accuracy: 0.6832\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1910 - accuracy: 0.9058 - val_loss: 0.5753 - val_accuracy: 0.6634\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1542 - accuracy: 0.9116 - val_loss: 0.7388 - val_accuracy: 0.7129\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1405 - accuracy: 0.9378 - val_loss: 0.8755 - val_accuracy: 0.6931\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1354 - accuracy: 0.9069 - val_loss: 0.2193 - val_accuracy: 0.8317\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0837 - accuracy: 0.9572 - val_loss: 0.4782 - val_accuracy: 0.7426\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1241 - accuracy: 0.8987 - val_loss: 0.3433 - val_accuracy: 0.8416\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.1152 - accuracy: 0.9228 - val_loss: 0.1937 - val_accuracy: 0.8713\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1128 - accuracy: 0.9111 - val_loss: 0.7307 - val_accuracy: 0.9010\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.0470 - accuracy: 0.9460 - val_loss: 0.5980 - val_accuracy: 0.8713\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1262 - accuracy: 0.9357 - val_loss: 0.4539 - val_accuracy: 0.9307\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.1123 - accuracy: 0.9392 - val_loss: 0.2924 - val_accuracy: 0.8614\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.1367 - accuracy: 0.9036 - val_loss: 0.2394 - val_accuracy: 0.8614\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1338 - accuracy: 0.9123 - val_loss: 0.3393 - val_accuracy: 0.8812\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 9s 583ms/step - loss: 0.1154 - accuracy: 0.9157 - val_loss: 0.2298 - val_accuracy: 0.9109\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0980 - accuracy: 0.9188 - val_loss: 0.3366 - val_accuracy: 0.8713\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.1325 - accuracy: 0.9241 - val_loss: 0.2570 - val_accuracy: 0.8713\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0921 - accuracy: 0.9427 - val_loss: 0.2317 - val_accuracy: 0.8812\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0806 - accuracy: 0.9308 - val_loss: 0.6113 - val_accuracy: 0.8515\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.0809 - accuracy: 0.9563 - val_loss: 0.3468 - val_accuracy: 0.8713\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0407 - accuracy: 0.9565 - val_loss: 1.0721 - val_accuracy: 0.8317\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0663 - accuracy: 0.9436 - val_loss: 0.4467 - val_accuracy: 0.9208\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0932 - accuracy: 0.9640 - val_loss: 0.2317 - val_accuracy: 0.8911\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 9s 576ms/step - loss: 0.1133 - accuracy: 0.9359 - val_loss: 1.1225 - val_accuracy: 0.7327\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0895 - accuracy: 0.9367 - val_loss: 1.1617 - val_accuracy: 0.7129\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0664 - accuracy: 0.9279 - val_loss: 0.3731 - val_accuracy: 0.8416\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 9s 576ms/step - loss: 0.0651 - accuracy: 0.9670 - val_loss: 2.8228 - val_accuracy: 0.8218\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0730 - accuracy: 0.9538 - val_loss: 1.5321 - val_accuracy: 0.8713\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0665 - accuracy: 0.9400 - val_loss: 4.6004 - val_accuracy: 0.8317\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0697 - accuracy: 0.9509 - val_loss: 1.4516 - val_accuracy: 0.8218\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.0630 - accuracy: 0.9330 - val_loss: 0.5941 - val_accuracy: 0.8218\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0893 - accuracy: 0.9320 - val_loss: 0.4496 - val_accuracy: 0.8218\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0901 - accuracy: 0.9209 - val_loss: 0.4700 - val_accuracy: 0.8119\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0418 - accuracy: 0.9524 - val_loss: 0.4536 - val_accuracy: 0.8713\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.1278 - accuracy: 0.9243 - val_loss: 1.2443 - val_accuracy: 0.8218\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0965 - accuracy: 0.9375 - val_loss: 0.4305 - val_accuracy: 0.8416\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0657 - accuracy: 0.9164 - val_loss: 0.4302 - val_accuracy: 0.8614\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0225 - accuracy: 0.9464 - val_loss: 0.2804 - val_accuracy: 0.8713\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0578 - accuracy: 0.9524 - val_loss: 0.3667 - val_accuracy: 0.9406\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0280 - accuracy: 0.9675 - val_loss: 0.3326 - val_accuracy: 0.9010\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0572 - accuracy: 0.9503 - val_loss: 0.3662 - val_accuracy: 0.8218\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 9s 585ms/step - loss: 0.0240 - accuracy: 0.9346 - val_loss: 0.3780 - val_accuracy: 0.8812\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 9s 583ms/step - loss: 0.0615 - accuracy: 0.9653 - val_loss: 0.3220 - val_accuracy: 0.8812\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1359 - accuracy: 0.9153 - val_loss: 0.3911 - val_accuracy: 0.8911\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.0848 - accuracy: 0.9306 - val_loss: 0.9833 - val_accuracy: 0.8416\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0589 - accuracy: 0.9507 - val_loss: 0.5120 - val_accuracy: 0.9109\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0227 - accuracy: 0.9706 - val_loss: 0.5892 - val_accuracy: 0.8911\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0149 - accuracy: 0.9531 - val_loss: 0.3691 - val_accuracy: 0.8713\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 9s 583ms/step - loss: 0.0422 - accuracy: 0.9567 - val_loss: 0.5487 - val_accuracy: 0.8416\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0334 - accuracy: 0.9441 - val_loss: 0.6087 - val_accuracy: 0.8020\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0663 - accuracy: 0.9322 - val_loss: 0.3723 - val_accuracy: 0.8416\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0432 - accuracy: 0.9466 - val_loss: 0.4566 - val_accuracy: 0.9109\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 9s 584ms/step - loss: 0.0508 - accuracy: 0.9457 - val_loss: 1.4734 - val_accuracy: 0.7327\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 9s 584ms/step - loss: 0.0696 - accuracy: 0.9218 - val_loss: 0.2235 - val_accuracy: 0.8515\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.1179 - accuracy: 0.8983 - val_loss: 0.5688 - val_accuracy: 0.8515\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.0545 - accuracy: 0.9264 - val_loss: 0.3894 - val_accuracy: 0.8317\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0158 - accuracy: 0.9290 - val_loss: 0.3107 - val_accuracy: 0.8317\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0297 - accuracy: 0.9180 - val_loss: 0.5417 - val_accuracy: 0.8614\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0069 - accuracy: 0.9255 - val_loss: 0.3874 - val_accuracy: 0.8416\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0144 - accuracy: 0.9234 - val_loss: 0.3524 - val_accuracy: 0.8317\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 9s 581ms/step - loss: 0.0088 - accuracy: 0.9460 - val_loss: 0.3375 - val_accuracy: 0.8515\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 9s 582ms/step - loss: 0.0423 - accuracy: 0.9105 - val_loss: 0.2995 - val_accuracy: 0.8416\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0295 - accuracy: 0.9036 - val_loss: 0.2835 - val_accuracy: 0.8614\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0606 - accuracy: 0.9303 - val_loss: 0.4383 - val_accuracy: 0.7921\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 9s 580ms/step - loss: 0.0539 - accuracy: 0.8709 - val_loss: 0.4398 - val_accuracy: 0.8416\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 9s 579ms/step - loss: 0.0347 - accuracy: 0.8970 - val_loss: 0.5284 - val_accuracy: 0.7921\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0299 - accuracy: 0.9359 - val_loss: 0.8851 - val_accuracy: 0.7129\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0189 - accuracy: 0.9266 - val_loss: 0.4281 - val_accuracy: 0.7921\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0173 - accuracy: 0.9571 - val_loss: 0.4052 - val_accuracy: 0.8515\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0217 - accuracy: 0.9482 - val_loss: 0.7426 - val_accuracy: 0.8614\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0298 - accuracy: 0.9681 - val_loss: 0.1739 - val_accuracy: 0.9010\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 9s 578ms/step - loss: 0.0257 - accuracy: 0.9334 - val_loss: 0.1827 - val_accuracy: 0.8812\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0295 - accuracy: 0.9289 - val_loss: 0.2402 - val_accuracy: 0.8416\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 9s 577ms/step - loss: 0.0192 - accuracy: 0.9211 - val_loss: 0.3097 - val_accuracy: 0.8515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f77e515c310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI8Do6Xm4yTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626e287f-5ecd-45bd-e88c-635c1ee8004a"
      },
      "source": [
        "model.evaluate(arr_inputs_float_test, arr_inputs_float_test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1851 - accuracy: 0.9459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18508212268352509, 0.9459459185600281]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss6wJ87AJQnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabda008-a762-4290-f51b-cbc13c34e2cd"
      },
      "source": [
        "predictions=model(arr_inputs_float_test)\n",
        "\n",
        "counter=0\n",
        "for i in range(len(targets_test)):\n",
        "  if targets_test[i][0]==1 and 0.5< predictions[i][0] and targets_test[i][1]==1 and 0.5< predictions[i][1]:  #[1,1]\n",
        "    counter+=1\n",
        "  if targets_test[i][0]==1 and 0.5< predictions[i][0] and targets_test[i][1]==0 and 0.5> predictions[i][1]:   #[1,0]\n",
        "    counter+=1\n",
        "  if targets_test[i][0]==0 and 0.5> predictions[i][0] and targets_test[i][1]==1 and 0.5< predictions[i][1]:    #[0,1]\n",
        "    counter+=1\n",
        "  if targets_test[i][0]==0 and 0.5> predictions[i][0] and targets_test[i][1]==0 and 0.5> predictions[i][1]:    #[0,0]\n",
        "    counter+=1\n",
        "\n",
        "\n",
        "print(counter/len(targets_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9459459459459459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuFkeT_kGqFH"
      },
      "source": [
        "RESNET 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUZvENSv5D9t",
        "outputId": "0a77ffb1-269d-429c-9a91-93beaa9970e5"
      },
      "source": [
        "datadir=\"/content/train/\" \n",
        "train_categories=['horse_train/', 'dolphin_train/', 'dolphin_horse_train/']\n",
        "\n",
        "dataset_train=[]\n",
        "def create_train_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    if category==\"horse_train/\":\n",
        "      class_num_train=[1,0]\n",
        "    if category==\"dolphin_train/\":\n",
        "      class_num_train=[0,1]\n",
        "    if category==\"dolphin_horse_train/\":\n",
        "      class_num_train=[1,1]\n",
        "    for img in os.listdir(path_train):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128)) \n",
        "        new_array1=new_array/255 \n",
        "        #scaling=StandardScaler()\n",
        "        #new_array1=scaling.fit_transform(new_array.reshape(-1, new_array.shape[-1])).reshape(new_array)\n",
        "        dataset_train.append([new_array1, class_num_train]) \n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_train.append([next(aug_image)[0], class_num_train])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_train_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_train)\n",
        "\n",
        "inputs_train=[]\n",
        "targets_train=[]\n",
        "\n",
        "for image, label in dataset_train:\n",
        "  inputs_train.append(image)\n",
        "  targets_train.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_train = np.array(inputs_train)\n",
        "arr_inputs_float_train=arr_inputs_train.astype('float32')\n",
        "\n",
        "\n",
        "arr_inputs_train_targets = np.array(targets_train)\n",
        "arr_inputs_float_train_targets=arr_inputs_train_targets.astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datadir=\"/content/valid/\" \n",
        "valid_categories=['horse_valid/', 'dolphin_valid/', 'dolphin_horse_valid/']\n",
        "\n",
        "dataset_valid=[]\n",
        "def create_validation_data():\n",
        "  for category in valid_categories:\n",
        "    path_valid=os.path.join(datadir, category)\n",
        "    if category==\"horse_valid/\":\n",
        "      class_num_valid=[1,0]\n",
        "    if category==\"dolphin_valid/\":\n",
        "      class_num_valid=[0,1]\n",
        "    if category==\"dolphin_horse_valid/\":\n",
        "      class_num_valid=[1,1]\n",
        "    for img in os.listdir(path_valid):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_valid, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))\n",
        "        new_array1=new_array/255 \n",
        "        #scaling=StandardScaler()\n",
        "        #new_array1=scaling.fit_transform(new_array.reshape(-1, new_array.shape[-1])).reshape(new_array)\n",
        "        dataset_valid.append([new_array1, class_num_valid])  \n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_valid.append([next(aug_image)[0], class_num_valid])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_validation_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_valid)\n",
        "\n",
        "inputs_val=[]\n",
        "targets_val=[]\n",
        "\n",
        "for image, label in dataset_valid:\n",
        "  inputs_val.append(image)\n",
        "  targets_val.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_val = np.array(inputs_val)\n",
        "arr_inputs_float_val=arr_inputs_val.astype('float32')\n",
        "\n",
        "arr_inputs_val_targets = np.array(targets_val)\n",
        "arr_inputs_float_val_targets=arr_inputs_val_targets.astype('float32')\n",
        "\n",
        "\n",
        "datadir=\"/content/test/\" \n",
        "test_categories=['HorseTest/', 'DolphinTest/', 'DolphinHorseTest/']\n",
        "\n",
        "dataset_test=[]\n",
        "def create_test_data():\n",
        "  for category in test_categories:\n",
        "    path_test=os.path.join(datadir, category)\n",
        "    if category==\"HorseTest/\":\n",
        "      class_num_test=[1,0]\n",
        "    if category==\"DolphinTest/\":\n",
        "      class_num_test=[0,1]\n",
        "    if category==\"DolphinHorseTest/\":\n",
        "      class_num_test=[1,1]\n",
        "    for img in os.listdir(path_test):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(path_test, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128)) \n",
        "        new_array1=new_array/255 \n",
        "        #scaling=StandardScaler()\n",
        "        #scalers = {}\n",
        "        #new_array1=scaling.fit_transform(new_array.reshape(-1, new_array.shape[-1])).reshape(new_array)\n",
        "        dataset_test.append([new_array1, class_num_test])\n",
        "        #image2 = np.expand_dims(new_array, axis=0)\n",
        "        #aug_image=gen.flow(image2) \n",
        "        #dataset_test.append([next(aug_image)[0], class_num_test])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_test_data()\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_test)\n",
        "\n",
        "inputs_test=[]\n",
        "targets_test=[]\n",
        "\n",
        "for image, label in dataset_test:\n",
        "  inputs_test.append(image)\n",
        "  targets_test.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "arr_inputs_test = np.array(inputs_test)\n",
        "arr_inputs_float_test=arr_inputs_test.astype('float32')\n",
        "\n",
        "arr_inputs_test_targets = np.array(targets_test)\n",
        "arr_inputs_float_test_targets=arr_inputs_test_targets.astype('float32')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqVuHEzHGouY"
      },
      "source": [
        "\n",
        "\n",
        "def convolutional_block(X, F1):\n",
        "    X = Conv2D(F1, (3, 3), strides = (1,1), padding='same')(X) \n",
        "    X = BatchNormalization(axis = 3)(X)  \n",
        "    X = Activation('relu')(X)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ResNet9(input_shape=(128, 128, 3), classes=2):\n",
        "  X_input = Input(input_shape)\n",
        "  X = convolutional_block(X_input, 64)\n",
        "  X = MaxPooling2D((2, 2))(X)                   #   64 x 64 x 64\n",
        "  X = convolutional_block(X, 128) \n",
        "  X = MaxPooling2D((2, 2))(X)                 # 128 x 32 x 32\n",
        "  X = convolutional_block(X, 128)\n",
        "  X = Add()([X, convolutional_block(X, 128)])\n",
        "  X = convolutional_block(X, 256)\n",
        "  X = MaxPooling2D((2, 2))(X)                # 256 x 16 x 16\n",
        "  X = convolutional_block(X, 512)\n",
        "  X = MaxPooling2D((2, 2))(X)         # 512 x 8 x 8\n",
        "  X = convolutional_block(X, 512)\n",
        "  X = Add()([X, convolutional_block(X, 512)])\n",
        "  X = MaxPooling2D((2, 2))(X)                             #  4 x 4\n",
        "  X = MaxPooling2D((4, 4))(X)   \n",
        "\n",
        "  # output layer\n",
        "  X = Flatten()(X)\n",
        "  X=tf.keras.layers.Dropout(0.2)(X)\n",
        "  #X=Dropout(0.2)(X)\n",
        "  X=Dense(1024, activation='relu')(X)\n",
        "  X=Dense(512, activation='relu')(X)\n",
        "  X = Dense(classes, activation='sigmoid')(X)\n",
        "\n",
        "\n",
        "  # Create model\n",
        "  model = Model(inputs = X_input, outputs = X, name='ResNet9')\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqIQ89qSGnu1"
      },
      "source": [
        "model = ResNet9(input_shape = (128, 128, 3), classes = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG4tz-uV8D3T",
        "outputId": "455a0b3c-4d7d-47d9-9149-5fa96c80d971"
      },
      "source": [
        "optim = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optim, loss='BinaryCrossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(arr_inputs_float_train, arr_inputs_float_train_targets,\n",
        "          validation_data=(arr_inputs_float_val, arr_inputs_float_val_targets),\n",
        "          epochs = 10, batch_size = 64)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 2s 174ms/step - loss: 1.7612e-05 - accuracy: 0.9300 - val_loss: 0.4974 - val_accuracy: 0.8218\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 1.1369e-05 - accuracy: 0.8979 - val_loss: 0.9401 - val_accuracy: 0.8218\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 0.0038 - accuracy: 0.9156 - val_loss: 1.4013 - val_accuracy: 0.7921\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 0.0240 - accuracy: 0.9464 - val_loss: 1.0054 - val_accuracy: 0.7921\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 0.0148 - accuracy: 0.9118 - val_loss: 0.8221 - val_accuracy: 0.8020\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 0.0201 - accuracy: 0.9070 - val_loss: 0.7080 - val_accuracy: 0.8812\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 0.0452 - accuracy: 0.9550 - val_loss: 1.7493 - val_accuracy: 0.7822\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 0.1160 - accuracy: 0.8973 - val_loss: 0.6351 - val_accuracy: 0.7921\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 0.0024 - accuracy: 0.9088 - val_loss: 0.8002 - val_accuracy: 0.8416\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 0.0040 - accuracy: 0.9075 - val_loss: 0.6147 - val_accuracy: 0.8416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9417c49610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsfbYPV96tYS",
        "outputId": "105e2a2c-48fe-4de9-fbeb-aac65d55ee3d"
      },
      "source": [
        "model.evaluate(arr_inputs_float_test, arr_inputs_float_test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 1s 188ms/step - loss: 0.3249 - accuracy: 0.9324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3249107897281647, 0.9324324131011963]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoAwOF8Z6zPO",
        "outputId": "f5a15203-702b-49d8-be3f-bb006d2825d3"
      },
      "source": [
        "predictions=model(arr_inputs_float_test)\n",
        "\n",
        "counter=0\n",
        "for i in range(len(targets_test)):\n",
        "  if targets_test[i][0]==1 and 0.5< predictions[i][0] and targets_test[i][1]==1 and 0.5< predictions[i][1]:  #[1,1]\n",
        "    counter+=1\n",
        "  if targets_test[i][0]==1 and 0.5< predictions[i][0] and targets_test[i][1]==0 and 0.5> predictions[i][1]:   #[1,0]\n",
        "    counter+=1\n",
        "  if targets_test[i][0]==0 and 0.5> predictions[i][0] and targets_test[i][1]==1 and 0.5< predictions[i][1]:    #[0,1]\n",
        "    counter+=1\n",
        "  if targets_test[i][0]==0 and 0.5> predictions[i][0] and targets_test[i][1]==0 and 0.5> predictions[i][1]:    #[0,0]\n",
        "    counter+=1\n",
        "\n",
        "\n",
        "print(counter/len(targets_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9324324324324325\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}