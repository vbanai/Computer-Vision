{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Resnet9/Transfer L_Horse_Dolphin.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbanai/Multi-label-classification_DOLPHIN_HORSE_Part1_Pytorch-Conv2DandResnet9_Tensorflow_DNNandCNN/blob/main/Pytorch_Resnet9_Transfer_L_Horse_Dolphin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNuyrZxiwv2U"
      },
      "source": [
        "# -------------   HORSE/DOLPHIN EXECRCISE WITH RESNET 9 ARCHITECTURE (MULTILABEL) +Data Augmentation----------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import cv2\n",
        "from torchvision.datasets import MNIST\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZlh0GzzWoN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c36af9-09c7-46cc-856f-079e37469715"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdvpmy4TbSPl"
      },
      "source": [
        "!ls \"/content/valid\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHCvz7CTfjOV"
      },
      "source": [
        "#delete folder with files\n",
        "!rm -rf \"/content/test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiHtTuV2zCLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926ef282-3476-4a8e-d31a-567da5cc4275"
      },
      "source": [
        "path=\".\"\n",
        "os.chdir(path)\n",
        "os.makedirs(\"train\")\n",
        "os.makedirs(\"valid\")\n",
        "os.makedirs(\"test\")\n",
        "\n",
        "path=\"./train\"\n",
        "os.chdir(path)\n",
        "os.makedirs(\"horse\")\n",
        "os.makedirs(\"dolphin\")\n",
        "os.makedirs(\"dolphin_horse\")\n",
        "\n",
        "path=\"/content/valid\"\n",
        "os.chdir(path)\n",
        "os.makedirs(\"horse\")\n",
        "os.makedirs(\"dolphin\")\n",
        "os.makedirs(\"dolphin_horse\")\n",
        "\n",
        "path=\"/content/test\"\n",
        "os.chdir(path)\n",
        "os.makedirs(\"horse\")\n",
        "os.makedirs(\"dolphin\")\n",
        "os.makedirs(\"dolphin_horse\")\n",
        "\n",
        "!pip install pyunpack\n",
        "!pip install patool\n",
        "from pyunpack import Archive\n",
        "Archive('/content/drive/MyDrive/dolphin_train.rar').extractall('/content/train/dolphin/')\n",
        "Archive('/content/drive/MyDrive/horse_train.rar').extractall('/content/train/horse/')\n",
        "Archive(\"/content/drive/MyDrive/dolphin_horse_train.rar\").extractall('/content/train/dolphin_horse/')\n",
        "Archive('/content/drive/MyDrive/horse_valid.rar').extractall('/content/valid/horse/')\n",
        "Archive('/content/drive/MyDrive/dolphin_valid.rar').extractall('/content/valid/dolphin/')\n",
        "Archive('/content/drive/MyDrive/dolphin_horse_valid.rar').extractall('/content/valid/dolphin_horse/')\n",
        "Archive('/content/drive/MyDrive/DolphinTest.rar').extractall('/content/test/dolphin/')\n",
        "Archive('/content/drive/MyDrive/HorseTest.rar').extractall('/content/test/horse/')\n",
        "Archive(\"/content/drive/MyDrive/DolphinHorseTest.rar\").extractall('/content/test/dolphin_horse/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyunpack\n",
            "  Downloading https://files.pythonhosted.org/packages/83/29/020436b1d8e96e5f26fa282b9c3c13a3b456a36b9ea2edc87c5fed008369/pyunpack-0.2.2-py2.py3-none-any.whl\n",
            "Collecting entrypoint2\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/b0/8ef4b1d8be02448d164c52466530059d7f57218655d21309a0c4236d7454/entrypoint2-0.2.4-py3-none-any.whl\n",
            "Collecting easyprocess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: entrypoint2, easyprocess, pyunpack\n",
            "Successfully installed easyprocess-0.3 entrypoint2-0.2.4 pyunpack-0.2.2\n",
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOeLiCA-rkVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4d97a7-3823-457d-8157-0f6ddcaf425e"
      },
      "source": [
        "#------------------ STAT FOR DOLPHIN TRAIN------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/train/dolphin/dolphin_train\" \n",
        "\n",
        "\n",
        "#datadir=\"/content/train/\" \n",
        "#train_categories=['horse/horse_train/', 'dolphin/dolphin_train/', 'dolphin_horse/dolphin_horse_train/']\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " # for category in train_categories:\n",
        "  #  path_train=os.path.join(datadir, category)\n",
        "  for img in os.listdir(datadir):  \n",
        "    try:\n",
        "      img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "      new_array=cv2.resize(img_array, (128, 128))  \n",
        "      new_array1=new_array/255 \n",
        "      new_array2=transform(new_array1)\n",
        "      dataset_stat.append(new_array2)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "\n",
        "stat_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(stat_train_loader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.6159, 0.5451, 0.4075], dtype=torch.float64),\n",
              " tensor([0.1874, 0.1824, 0.1982], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV-QxOsR9_Rn",
        "outputId": "cc2091e5-e560-42a7-ebcd-c73a6e25bd40"
      },
      "source": [
        "\n",
        "##########  -------------------DOLPHIN SET TRAIN----------------------\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "stats = ((0.6159, 0.5451, 0.4075), (0.1874, 0.1824, 0.1982))\n",
        "train_tfms2 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         transforms.RandomHorizontalFlip(),\n",
        "                         transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "                   \n",
        "datadir=\"/content/train/\" \n",
        "train_categories=['horse/horse_train/', 'dolphin/dolphin_train/', 'dolphin_horse/dolphin_horse_train/']\n",
        "\n",
        "dataset_train=[]\n",
        "def create_train_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    #if category==\"horse/horse_train/\":\n",
        "     # class_num_train=torch.tensor([1,0], dtype=torch.float32)\n",
        "    if category==\"dolphin/dolphin_train/\":\n",
        "      class_num_train=torch.tensor([0,1], dtype=torch.float32)\n",
        "    #if category==\"dolphin_horse/dolphin_horse_train/\":\n",
        "    #  class_num_train=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_train):  \n",
        "        try:   \n",
        "          img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "          #img_array2=np.array(img_array, dtype=np.float32)\n",
        "          #new_array=cv2.resize(img_array, (128, 128))  \n",
        "          #new_array0=img_array/255 \n",
        "          new_array1=train_tfms2(img_array)\n",
        "          #new_array2=new_array1/255\n",
        "          #new_array3=torch.tensor(new_array2, dtype=torch.float32)\n",
        "          dataset_train.append([new_array1, class_num_train])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_train_data()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pic should be Tensor or ndarray. Got <class 'NoneType'>.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONlqrNIho4sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abfdc8c-ba8d-4cb2-dfd8-ed4c6bcac87f"
      },
      "source": [
        "#------------------ Horse STAT TRAIN-------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/train/horse/horse_train\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "horse_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(horse_train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4061, 0.4831, 0.5015], dtype=torch.float64),\n",
              " tensor([0.2390, 0.2349, 0.2310], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC70UP80r-M_",
        "outputId": "0ab4bf56-3308-4f21-d908-ffc470d1261f"
      },
      "source": [
        "##########  -------------------HORSE SET TRAIN----------------------\n",
        "\n",
        "#from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "stats = ((0.4061, 0.4831, 0.5015), (0.2390, 0.2349, 0.2310))\n",
        "\n",
        "train_tfms2 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         transforms.RandomHorizontalFlip(),\n",
        "                         transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "                   \n",
        "datadir=\"/content/train/\" \n",
        "train_categories=['horse/horse_train/', 'dolphin/dolphin_train/', 'dolphin_horse/dolphin_horse_train/']\n",
        "\n",
        "\n",
        "def create_train_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    if category==\"horse/horse_train/\":\n",
        "      class_num_train=torch.tensor([1,0], dtype=torch.float32)\n",
        "    #if category==\"dolphin/dolphin_train/\":\n",
        "    #  class_num_train=torch.tensor([0,1], dtype=torch.float32)\n",
        "    #if category==\"dolphin_horse/dolphin_horse_train/\":\n",
        "     # class_num_train=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_train):  \n",
        "        try:   \n",
        "          img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "          #img_array2=np.array(img_array, dtype=np.float32)\n",
        "          #new_array=cv2.resize(img_array, (128, 128))  \n",
        "          #new_array0=img_array/255 \n",
        "          new_array1=train_tfms2(img_array)\n",
        "          #new_array2=new_array1/255\n",
        "          #new_array3=torch.tensor(new_array2, dtype=torch.float32)\n",
        "          dataset_train.append([new_array1, class_num_train])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_train_data()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pic should be Tensor or ndarray. Got <class 'NoneType'>.\n",
            "pic should be Tensor or ndarray. Got <class 'NoneType'>.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeBWGRtQtCOe"
      },
      "source": [
        "#-------------- STAT FOR HORSE&DOLPHIN TRAIN------\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/train/dolphin_horse/dolphin_horse_train\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "horse_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(horse_train_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV7hD3dotjhZ"
      },
      "source": [
        "###########------- HORSE&DOLPHIN TRAIN DATA-------\n",
        "\n",
        "\n",
        "stats = ((0.6082, 0.5902, 0.5349), (0.2793, 0.2695, 0.3072))\n",
        "train_tfms2 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         transforms.RandomHorizontalFlip(),\n",
        "                         transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "                   \n",
        "datadir=\"/content/train/\" \n",
        "train_categories=['horse/horse_train/', 'dolphin/dolphin_train/', 'dolphin_horse/dolphin_horse_train/']\n",
        "\n",
        "\n",
        "def create_train_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    #if category==\"horse/horse_train/\":\n",
        "     # class_num_train=torch.tensor([1,0], dtype=torch.float32)\n",
        "    #if category==\"dolphin/dolphin_train/\":\n",
        "    #  class_num_train=torch.tensor([0,1], dtype=torch.float32)\n",
        "    if category==\"dolphin_horse/dolphin_horse_train/\":\n",
        "      class_num_train=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_train):  \n",
        "        try:   \n",
        "          img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "          #img_array2=np.array(img_array, dtype=np.float32)\n",
        "          #new_array=cv2.resize(img_array, (128, 128))  \n",
        "          #new_array0=img_array/255 \n",
        "          new_array1=train_tfms2(img_array)\n",
        "          #new_array2=new_array1/255\n",
        "          #new_array3=torch.tensor(new_array2, dtype=torch.float32)\n",
        "          dataset_train.append([new_array1, class_num_train])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_train_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clu5jXc3CbXd"
      },
      "source": [
        "#########------Final TRAIN DATASET CREATION------\n",
        "\n",
        "import random\n",
        "random.shuffle(dataset_train)\n",
        "\n",
        "inputs_train=[]\n",
        "targets_train0=[]\n",
        "\n",
        "for image, label in dataset_train:\n",
        "  inputs_train.append(image)\n",
        "  targets_train0.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "#arr_inputs_train = np.array(inputs_train)\n",
        "#arr_inputs_float_train=arr_inputs_train.astype('float32')\n",
        "\n",
        "\n",
        "#---convert np array of the input images to tensors-----\n",
        "#arr_tensor_train = [torch.from_numpy(item).float() for item in arr_inputs_float_train]\n",
        "arr_stack_train=torch.stack(inputs_train)\n",
        "\n",
        "#--- reshape the input images tensor\n",
        "arr_stack_train_reshape=arr_stack_train.reshape(-1,3,128,128) \n",
        "targets_train=torch.stack(targets_train0)\n",
        "#targets_torch_train=torch.tensor(targets_train, dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWy2S5qNxAo9"
      },
      "source": [
        "---------------TRAIN FINAL END----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDTu2zmnKIrP"
      },
      "source": [
        "plt.imshow(train_final[2][0].permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84CYLCmLxbRe"
      },
      "source": [
        "-------VALIDATION DATASET------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTofOplx4Hha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc474986-6184-406a-c2c9-6b643fb437be"
      },
      "source": [
        "\n",
        "\n",
        "#------------------ STAT FOR VALID DOLPHIN-------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/valid/dolphin/dolphin_valid\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "horse_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(horse_train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.6489, 0.5334, 0.3283], dtype=torch.float64),\n",
              " tensor([0.1639, 0.1617, 0.1790], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAjoo-UYSeSP"
      },
      "source": [
        "###############----- Valid set creation - Dolphin-------\n",
        "\n",
        "stats = ((0.6489, 0.5334, 0.3283), (0.1639, 0.1617, 0.1790))\n",
        "\n",
        "train_tfms2 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         #transforms.RandomHorizontalFlip(),\n",
        "                         #transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "                   \n",
        "datadir=\"/content/valid/\" \n",
        "train_categories=['horse/horse_valid/', 'dolphin/dolphin_valid/', 'dolphin_horse/dolphin_horse_valid/']\n",
        "\n",
        "dataset_valid=[]\n",
        "def create_valid_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    #if category==\"horse/horse_valid/\":\n",
        "     # class_num_valid=torch.tensor([1,0], dtype=torch.float32)\n",
        "    if category==\"dolphin/dolphin_valid/\":\n",
        "      class_num_valid=torch.tensor([0,1], dtype=torch.float32)\n",
        "    #if category==\"dolphin_horse/dolphin_horse_valid/\":\n",
        "    #  class_num_valid=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_train):  \n",
        "        try:   \n",
        "          img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "          #img_array2=np.array(img_array, dtype=np.float32)\n",
        "          #new_array=cv2.resize(img_array, (128, 128))  \n",
        "          #new_array0=img_array/255 \n",
        "          new_array1=train_tfms2(img_array)\n",
        "          #new_array2=new_array1/255\n",
        "          #new_array3=torch.tensor(new_array2, dtype=torch.float32)\n",
        "          dataset_valid.append([new_array1, class_num_valid])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_valid_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39bc0zYi4fxi"
      },
      "source": [
        "#-------------STAT FOR HORSE VALIDATION DATASET---------\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/valid/horse/horse_valid\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "horse_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(horse_train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik57OkdMTScF"
      },
      "source": [
        "############### -----------Horse_validation set creation----------\n",
        "\n",
        "stats = ((0.3924, 0.5006, 0.5159), (0.2448, 0.2328, 0.2217))\n",
        "\n",
        "train_tfms2 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         #transforms.RandomHorizontalFlip(),\n",
        "                         #transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "                   \n",
        "datadir=\"/content/valid/\" \n",
        "train_categories=['horse/horse_valid/', 'dolphin/dolphin_valid/', 'dolphin_horse/dolphin_horse_valid/']\n",
        "\n",
        "\n",
        "def create_valid_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    if category==\"horse/horse_valid/\":\n",
        "      class_num_valid=torch.tensor([1,0], dtype=torch.float32)\n",
        "    #if category==\"dolphin/dolphin_valid/\":\n",
        "     # class_num_valid=torch.tensor([0,1], dtype=torch.float32)\n",
        "    #if category==\"dolphin_horse/dolphin_horse_valid/\":\n",
        "    #  class_num_valid=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_train):  \n",
        "        try:   \n",
        "          img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "          #img_array2=np.array(img_array, dtype=np.float32)\n",
        "          #new_array=cv2.resize(img_array, (128, 128))  \n",
        "          #new_array0=img_array/255 \n",
        "          new_array1=train_tfms2(img_array)\n",
        "          #new_array2=new_array1/255\n",
        "          #new_array3=torch.tensor(new_array2, dtype=torch.float32)\n",
        "          dataset_valid.append([new_array1, class_num_valid])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_valid_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSQCXtUK40sZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cd3851-1f2b-4aa3-ba3a-eeb419bd66de"
      },
      "source": [
        "#-------------STAT FOR HORSED/DOLPIN VALIDATION DATASET----------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/valid/dolphin_horse/dolphin_horse_valid\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "horse_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(horse_train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.6164, 0.5990, 0.5362], dtype=torch.float64),\n",
              " tensor([0.3014, 0.2810, 0.3143], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S9wy5nnVeI5"
      },
      "source": [
        "#############----------- DolphinHorse validity set creation----------------\n",
        "\n",
        "stats = ((0.6164, 0.5990, 0.5362), (0.3014, 0.2810, 0.3143))\n",
        "train_tfms2 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         #transforms.RandomHorizontalFlip(),\n",
        "                         #transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "                   \n",
        "datadir=\"/content/valid/\" \n",
        "train_categories=['horse/horse_valid/', 'dolphin/dolphin_valid/', 'dolphin_horse/dolphin_horse_valid/']\n",
        "\n",
        "\n",
        "def create_valid_data():\n",
        "  for category in train_categories:\n",
        "    path_train=os.path.join(datadir, category)\n",
        "    #if category==\"horse/horse_valid/\":\n",
        "     # class_num_valid=torch.tensor([1,0], dtype=torch.float32)\n",
        "    #if category==\"dolphin/dolphin_valid/\":\n",
        "     # class_num_valid=torch.tensor([0,1], dtype=torch.float32)\n",
        "    if category==\"dolphin_horse/dolphin_horse_valid/\":\n",
        "      class_num_valid=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_train):  \n",
        "        try:   \n",
        "          img_array=cv2.imread(os.path.join(path_train, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "          #img_array2=np.array(img_array, dtype=np.float32)\n",
        "          #new_array=cv2.resize(img_array, (128, 128))  \n",
        "          #new_array0=img_array/255 \n",
        "          new_array1=train_tfms2(img_array)\n",
        "          #new_array2=new_array1/255\n",
        "          #new_array3=torch.tensor(new_array2, dtype=torch.float32)\n",
        "          dataset_valid.append([new_array1, class_num_valid])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_valid_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYUhjXwZzy1W"
      },
      "source": [
        "#-----VAlIDTY FINAL-------\n",
        "import random\n",
        "random.shuffle(dataset_valid)\n",
        "\n",
        "inputs_valid=[]\n",
        "targets_valid0=[]\n",
        "\n",
        "for image, label in dataset_valid:\n",
        "  inputs_valid.append(image)\n",
        "  targets_valid0.append(label)\n",
        "\n",
        "#---creating np array from the input images\n",
        "#arr_inputs_train = np.array(inputs_train)\n",
        "#arr_inputs_float_train=arr_inputs_train.astype('float32')\n",
        "\n",
        "\n",
        "#---convert np array of the input images to tensors-----\n",
        "#arr_tensor_train = [torch.from_numpy(item).float() for item in arr_inputs_float_train]\n",
        "arr_stack_valid=torch.stack(inputs_valid)\n",
        "\n",
        "#--- reshape the input images tensor\n",
        "arr_stack_valid_reshape=arr_stack_valid.reshape(-1,3,128,128) \n",
        "targets_valid=torch.stack(targets_valid0)\n",
        "#targets_torch_train=torch.tensor(targets_train, dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSz_V1N5WFPY"
      },
      "source": [
        "batch_size=32\n",
        "\n",
        "dataset_tensor_train=TensorDataset(arr_stack_train_reshape, targets_train)\n",
        "dataset_tensor_valid=TensorDataset(arr_stack_valid_reshape, targets_valid)\n",
        "\n",
        "train_dl = DataLoader(dataset_tensor_train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dl = DataLoader(dataset_tensor_valid, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfRtAbmdqGyK"
      },
      "source": [
        "\n",
        "batch_size=32\n",
        "\n",
        "dataset=TensorDataset(arr_stack_train_reshape, targets_train)\n",
        "dataset_tensor_test=TensorDataset(arr_stack_valid_reshape, targets_valid)\n",
        "\n",
        "dataset_tensor_train, dataset_tensor_valid =train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "\n",
        "train_dl = DataLoader(dataset_tensor_train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dl = DataLoader(dataset_tensor_valid, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_dl=DataLoader(dataset_tensor_test, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQeXkqS6ONgA"
      },
      "source": [
        "stats = ((0.4079, 0.5456, 0.6163), (0.1924, 0.1763, 0.1816))\n",
        "\n",
        "def denormalize(images, means, stds):\n",
        "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
        "    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
        "    return images * stds + means\n",
        "\n",
        "example = iter(train_dl)\n",
        "sample, label = example.next()\n",
        "\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  denorm_images = denormalize(sample[i], *stats)\n",
        "  plt.imshow(sample[i].permute(1,2,0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdQWAJ4YJQCw"
      },
      "source": [
        "def show_batch(train_dl):\n",
        "    for images, labels in train_dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        #ax.set_xticks([]); ax.set_yticks([])\n",
        "        denorm_images = denormalize(images, *stats)\n",
        "        ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n",
        "        break\n",
        "\n",
        "show_batch(train_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy73ImKY6y9Z"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGeHOo4S9KDe",
        "outputId": "44092b32-a119-4eb5-977a-89596260d56c"
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODw-1wAHerRe"
      },
      "source": [
        "# ~ and &\n",
        "# ~ = NOT\n",
        "# & = AND different and /or\n",
        "\n",
        "\n",
        "# Bitwise NOT and bitwise and operator\n",
        "# Checkout https://realpython.com/python-bitwise-operators/ for detailed info\n",
        "threshold = 0.5\n",
        "output = np.array([[0.7, 0.6, 0.1, 0.7]])\n",
        "label = np.array([[1, 1, 1, 1]])\n",
        "# prob and label are List of BOOLEANs (True or False)\n",
        "prob = output > threshold #\n",
        "label = label > threshold\n",
        "\n",
        "print(prob, label)\n",
        "(prob & label).sum(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECLRzZzfuY3C"
      },
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "\n",
        "    # F2 = 2* precision * recall / (precision + recall)\n",
        "\n",
        "    return F2.mean(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO5vHCxPWnV1"
      },
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "  \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.binary_cross_entropy(out, labels)  # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)# Generate predictions\n",
        "        counter=0\n",
        "        for i in range(len(batch)):\n",
        "          if out[i][0]>=0.5 and labels[i][0]==1 and out[i][1]<0.5 and labels[i][1]==0:\n",
        "            counter+=1\n",
        "          if out[i][0]<0.5 and labels[i][0]==0 and out[i][1]>=0.5 and labels[i][1]==1:\n",
        "            counter+=1\n",
        "          if out[i][0]>=0.5 and labels[i][0]==1 and out[i][1]>=0.5 and labels[i][1]==1:\n",
        "            counter+=1\n",
        "        check=counter/len(batch)\n",
        "        loss = F.binary_cross_entropy(out, labels)    # Calculate loss\n",
        "        score = F_score(out, labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': score, 'Check': check}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        batch_check=[x['Check'] for x in outputs]\n",
        "        epoch_check=sum(batch_check)/len(batch_check)\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'Check':epoch_check}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, check: {}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc'], result['Check']))\n",
        "        \n",
        "\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Check out ResNeXT from torch vision:\n",
        "# https://learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/\n",
        "# NUS-WIDE dataset\n",
        "\n",
        "\n",
        "class ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64, pool=True)  #64 x64\n",
        "        self.conv2 = conv_block(64, 128, pool=True)          #32 x 32\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True) # 16 x 16\n",
        "        self.conv4 = conv_block(256, 512, pool=True) # 8 x 8\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(2, 2),\n",
        "                                        nn.MaxPool2d(4), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes),\n",
        "                                        nn.Sigmoid())\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yEnaUZwfxRw"
      },
      "source": [
        "# if GPU is not available\n",
        "model = ResNet9(3, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSKKS600XGYt"
      },
      "source": [
        "model = to_device(ResNet9(3, 2), device)\n",
        "model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74z8FxyJW9dl"
      },
      "source": [
        "history = []\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    #history = []\n",
        "    \n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                               steps_per_epoch=len(train_loader))\n",
        "    #sched = torch.optim.lr_scheduler.StepLR(opt_func, step_size=100, gamma=0.1)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    #return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2iIdWvR-FDi"
      },
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWvJaXreXAki"
      },
      "source": [
        "epochs = 40\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam\n",
        "\n",
        "\n",
        "# LR Scheduler (On Plateau)\n",
        "# Loss 0.001, 0.001..... 10 rounds --> Lr = multiplier*Lr\n",
        "\n",
        "#lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_func, patience=10)\n",
        "\n",
        "# LR Scheduler (STEP)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(opt_func, step_size=100, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C893pzA7A846"
      },
      "source": [
        "# Callbacks: Early Stopping callback\n",
        "# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Ao74Mq7R_O"
      },
      "source": [
        "%%time\n",
        "fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APOnRjWmlySv"
      },
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');\n",
        "\n",
        "plot_accuracies(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jneGYv1TqYwk"
      },
      "source": [
        "\n",
        "train_losses = [x.get('train_loss') for x in history]\n",
        "val_losses = [x['val_loss'] for x in history]\n",
        "plt.plot(train_losses, '-bx')\n",
        "plt.plot(val_losses, '-rx')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.title('Loss vs. No. of epochs');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Pw5btkrABo"
      },
      "source": [
        "\n",
        "lrs = np.concatenate([x['lrs'] for x in history])\n",
        "plt.plot(lrs)\n",
        "plt.xlabel('Batch no.')\n",
        "plt.ylabel('Learning rate')\n",
        "plt.title('Learning Rate vs. Batch no.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnTKZQ-Mwsgi"
      },
      "source": [
        "torch.save(model.state_dict(), 'pics.modelparameters')\n",
        "model2=ResNet9(3,2)\n",
        "model2.load_state_dict(torch.load('pics.modelparameters'))\n",
        "model2.state_dict\n",
        "\n",
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    #_, preds  = torch.max(yb, dim=1)\n",
        "    return yb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2IXWY9bRmyL",
        "outputId": "25fce40b-45f8-480d-89b9-e9c9a5630cb9"
      },
      "source": [
        "#------------------ STAT FOR TEST DOLPHIN------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/test/dolphin/DolphinTest\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "\n",
        "stat_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(stat_train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.6165, 0.5122, 0.2994], dtype=torch.float64),\n",
              " tensor([0.1571, 0.1548, 0.1889], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTCbQA_3EmTz"
      },
      "source": [
        "###########----TEST DATASET DOLPHIN CREATION-----\n",
        "\n",
        "stats = ((0.6165, 0.5122, 0.2994), (0.1571, 0.1548, 0.1889))\n",
        "test_tfms3 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         #transforms.RandomHorizontalFlip(),\n",
        "                         #transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "\n",
        "\n",
        "dir_test=\"/content/test\" \n",
        "test_categories=['horse/HorseTest/', 'dolphin/DolphinTest/', 'dolphin_horse/DolphinHorseTest/']\n",
        "test_list=[]\n",
        "\n",
        "def create_test_data():\n",
        "  for category in test_categories:\n",
        "    path_test=os.path.join(dir_test, category)\n",
        "    #if category==\"horse/HorseTest/\":\n",
        "     # class_num_test=torch.tensor([1,0], dtype=torch.float32)\n",
        "    if category==\"dolphin/DolphinTest/\":\n",
        "      class_num_test=torch.tensor([0,1], dtype=torch.float32)\n",
        "    #if category==\"dolphin_horse/DolphinHorseTest/\":\n",
        "     # class_num_test=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_test):  \n",
        "        try:\n",
        "          img_array=cv2.imread(os.path.join(path_test, img))  \n",
        "          new_array0=test_tfms3(img_array) \n",
        "          #new_array=cv2.resize(img_array, (128, 128))\n",
        "          #new_array1=new_array0/255 \n",
        "          #test_np = np.array(new_array1)\n",
        "          #test_np_float32=test_np.astype('float32')\n",
        "          #test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "          #test_tensor=new_array0.reshape(3,128,128)\n",
        "          test_list.append([new_array0, class_num_test])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-nDDG0vYxUp",
        "outputId": "05693089-4fc0-4823-d05b-760c3f522411"
      },
      "source": [
        "#------------------ STAT FOR TEST HORSE------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/test/horse/HorseTest\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "\n",
        "stat_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(stat_train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4448, 0.5173, 0.5299], dtype=torch.float64),\n",
              " tensor([0.2385, 0.2264, 0.2245], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx4Ro9G5ZvKv"
      },
      "source": [
        "################----TEST DATASET HORSE CREATION-----\n",
        "\n",
        "stats = ((0.4448, 0.5173, 0.5299), (0.2385, 0.2264, 0.2245))\n",
        "test_tfms3 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         #transforms.RandomHorizontalFlip(),\n",
        "                         #transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "\n",
        "\n",
        "dir_test=\"/content/test\" \n",
        "test_categories=['horse/HorseTest/', 'dolphin/DolphinTest/', 'dolphin_horse/DolphinHorseTest/']\n",
        "\n",
        "\n",
        "def create_test_data():\n",
        "  for category in test_categories:\n",
        "    path_test=os.path.join(dir_test, category)\n",
        "    if category==\"horse/HorseTest/\":\n",
        "      class_num_test=torch.tensor([1,0], dtype=torch.float32)\n",
        "    #if category==\"dolphin/DolphinTest/\":\n",
        "     # class_num_test=torch.tensor([0,1], dtype=torch.float32)\n",
        "    #if category==\"dolphin_horse/DolphinHorseTest/\":\n",
        "     # class_num_test=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_test):  \n",
        "        try:\n",
        "          img_array=cv2.imread(os.path.join(path_test, img))  \n",
        "          new_array0=test_tfms3(img_array) \n",
        "          #new_array=cv2.resize(img_array, (128, 128))\n",
        "          #new_array1=new_array0/255 \n",
        "          #test_np = np.array(new_array1)\n",
        "          #test_np_float32=test_np.astype('float32')\n",
        "          #test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "          #test_tensor=new_array0.reshape(3,128,128)\n",
        "          test_list.append([new_array0, class_num_test])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ0HCYWQZTKx",
        "outputId": "5fcb3380-7050-466b-a8fe-9790d0ba1ede"
      },
      "source": [
        "#------------------ STAT FOR TEST HORSEDOLPHIN------\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor() ])\n",
        "\n",
        "datadir=\"/content/test/dolphin_horse/DolphinHorseTest\" \n",
        "\n",
        "\n",
        "dataset_stat=[]\n",
        "def create_stat_data():\n",
        " \n",
        "    for img in os.listdir(datadir):  \n",
        "      try:\n",
        "        img_array=cv2.imread(os.path.join(datadir, img))#, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array=cv2.resize(img_array, (128, 128))  \n",
        "        new_array1=new_array/255 \n",
        "        new_array2=transform(new_array1)\n",
        "        dataset_stat.append(new_array2)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "create_stat_data()\n",
        "arr_stack_stack=torch.stack(dataset_stat)\n",
        "\n",
        "\n",
        "stat_train_loader=DataLoader(arr_stack_stack, batch_size=32, shuffle=False)\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "  mean=0.\n",
        "  std=0.\n",
        "  total_images_count=0\n",
        "  for images in loader:\n",
        "    image_count_in_a_bach=images.size(0)\n",
        "    images=images.view(image_count_in_a_bach, images.size(1), -1)\n",
        "    mean +=images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_images_count +=image_count_in_a_bach\n",
        "\n",
        "  mean /=total_images_count\n",
        "  std /=total_images_count\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "get_mean_and_std(stat_train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.6090, 0.6004, 0.5262], dtype=torch.float64),\n",
              " tensor([0.2978, 0.2828, 0.3395], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlpG6PwWZCsv"
      },
      "source": [
        "###############----TEST DATASET DOLPHIN_HORSE CREATION-----\n",
        "\n",
        "stats = ((0.6090, 0.6004, 0.5262), (0.2978, 0.2828, 0.3395))\n",
        "test_tfms3 = transforms.Compose([\n",
        "                         transforms.ToPILImage(),\n",
        "                         transforms.Resize((128, 128)),\n",
        "                         #transforms.RandomHorizontalFlip(),\n",
        "                         #transforms.RandomCrop(128, padding=4, padding_mode='reflect'),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats,inplace=True)]) \n",
        "\n",
        "\n",
        "dir_test=\"/content/test\" \n",
        "test_categories=['horse/HorseTest/', 'dolphin/DolphinTest/', 'dolphin_horse/DolphinHorseTest/']\n",
        "\n",
        "\n",
        "def create_test_data():\n",
        "  for category in test_categories:\n",
        "    path_test=os.path.join(dir_test, category)\n",
        "    #if category==\"horse/HorseTest/\":\n",
        "     # class_num_test=torch.tensor([1,0], dtype=torch.float32)\n",
        "    #if category==\"dolphin/DolphinTest/\":\n",
        "     # class_num_test=torch.tensor([0,1], dtype=torch.float32)\n",
        "    if category==\"dolphin_horse/DolphinHorseTest/\":\n",
        "      class_num_test=torch.tensor([1,1], dtype=torch.float32)\n",
        "      for img in os.listdir(path_test):  \n",
        "        try:\n",
        "          img_array=cv2.imread(os.path.join(path_test, img))  \n",
        "          new_array0=test_tfms3(img_array) \n",
        "          #new_array=cv2.resize(img_array, (128, 128))\n",
        "          #new_array1=new_array0/255 \n",
        "          #test_np = np.array(new_array1)\n",
        "          #test_np_float32=test_np.astype('float32')\n",
        "          #test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "          #test_tensor=new_array0.reshape(3,128,128)\n",
        "          test_list.append([new_array0, class_num_test])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "create_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpfviEWYech"
      },
      "source": [
        "inputs_test=[]\n",
        "targets_test0=[]\n",
        "\n",
        "for image, label in test_list:\n",
        "  inputs_test.append(image)\n",
        "  targets_test0.append(label)\n",
        "\n",
        "stack_test_list=torch.stack(inputs_test)\n",
        "stack_test_targets=torch.stack(targets_test0)\n",
        "arr_stack_test_reshape=stack_test_list.reshape(-1,3,128,128) \n",
        "\n",
        "batch_size=32\n",
        "\n",
        "\n",
        "dataset_tensor_test=TensorDataset(arr_stack_test_reshape, stack_test_targets)\n",
        "test_dl = DataLoader(dataset_tensor_test, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_dl2=DataLoader(arr_stack_test_reshape, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "#test_dl2device=DeviceDataLoader(test_dl, device)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcIkfemnr-yx",
        "outputId": "13f0bd83-bf43-44c9-963b-ab4260e55a2a"
      },
      "source": [
        "\n",
        "result_test=evaluate(model2, test_dl)\n",
        "result_test\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Check': 1.0, 'val_acc': 0.9947090148925781, 'val_loss': 0.039370790123939514}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJYo2R33UYD8",
        "outputId": "a48d7853-5af9-4e78-c53a-1d2d5a896e90"
      },
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "\n",
        "    # F2 = 2* precision * recall / (precision + recall)\n",
        "\n",
        "    return F2.mean(0)\n",
        "test=torch.from_numpy(test_numpy)\n",
        "F_score(test, stack_test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7928)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4MSVdaSWYWu",
        "outputId": "e7250171-d39a-41f9-dc3a-efdf08aeec16"
      },
      "source": [
        "print(test_numpy[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.98804444 0.98063207]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLZi8s9XVl-k"
      },
      "source": [
        "counter=0\n",
        "for i in range(len(test_list)):\n",
        "  print(test_list[i][1], test_numpy[i])\n",
        "  if test_list[i][1][0]==0 and test_numpy[i][0]<0.5 and test_list[i][1][1]==1 and test_numpy[i][1]>=0.5:\n",
        "    counter+=1\n",
        "    print(test_list[i][1][0], test_numpy[i][0], test_list[i][1][1], test_numpy[i][1])\n",
        "  if test_list[i][1][0]==1 and test_numpy[i][0]>=0.5 and test_list[i][1][1]==0 and test_numpy[i][1]<0.5:\n",
        "    counter+=1\n",
        "    print(test_list[i][1][0], test_numpy[i][0], test_list[i][1][1], test_numpy[i][1])\n",
        "  if test_list[i][1][0]==1 and test_numpy[i][0]>=0.5 and test_list[i][1][1]==1 and test_numpy[i][1]>=0.5:\n",
        "    counter+=1\n",
        "    print(test_list[i][1][0], test_numpy[i][0], test_list[i][1][1], test_numpy[i][1])\n",
        "\n",
        "result=counter/len(test_list)  \n",
        "print(counter)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AaTDTO1RG60"
      },
      "source": [
        "stack_test_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pRuHJHksAzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149cb276-bfac-4370-9bec-65dd1bbe8ec5"
      },
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "\n",
        "    # F2 = 2* precision * recall / (precision + recall)\n",
        "\n",
        "    return F2.mean(0)\n",
        "\n",
        "for batch in test_dl:\n",
        "  inputs, labels=batch\n",
        "  a=model2(inputs)\n",
        "  score = F_score(a, labels)\n",
        "  print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7074)\n",
            "tensor(0.8113)\n",
            "tensor(0.9189)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH6BwHBKSSxA"
      },
      "source": [
        "a=result_test[0][0][0].detach().numpy()\n",
        "counterH=0\n",
        "counterD=0\n",
        "counterHD=0\n",
        "for x in range(len(test_result2)):\n",
        "  for i in range(len(result[x])):\n",
        "    if result[0][i][0].detach().numpy()>0.5 and result[0][i][1].detach().numpy()<0.5:\n",
        "      counterH+=1\n",
        "    if result[0][i][0].detach().numpy()<0.5 and result[0][i][1].detach().numpy()>0.5:\n",
        "      counterD+=1\n",
        "    if result[0][i][0].detach().numpy()>0.5 and result[0][i][1].detach().numpy()>0.5:\n",
        "      counterHD+=1\n",
        "\n",
        "print(\"Horse= \", counterH)\n",
        "print(\"Dolphin= \", counterD)\n",
        "print(\"HoreseDolphin= \", counterHD)\n",
        "#horse=35 Dolphin=27 HorseDolphin=12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C34ANcAdE8bV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1093abd8-a894-40e0-d341-20dedc1c6faf"
      },
      "source": [
        "#Prediction for image containing just dolphin (it should be around [0,1])\n",
        "\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/tesztd.jpg\")#, cv2.IMREAD_GRAYSCALE)\n",
        "#new_array=cv2.resize(img_array, (128, 128))\n",
        "new_array0=test_tfms3(img_array) \n",
        "#new_array1=new_array/255 \n",
        "#test_np = np.array(new_array1)\n",
        "#test_np_float32=test_np.astype('float32')\n",
        "#test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "test_tensor=new_array0.reshape(3,128,128)\n",
        "\n",
        "predict_image(test_tensor, model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0010, 0.9997]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHw2oUuImbuG",
        "outputId": "9fe7338c-a465-4935-f18d-cd87517bb559"
      },
      "source": [
        "image2device=to_device(test_tensor.unsqueeze(0), device)\n",
        "model(image2device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1.]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP8rk26sFZnl",
        "outputId": "9c4ef580-a655-4a08-e4cc-602ed5ff851f"
      },
      "source": [
        "#Prediction for image containing just horse (it should be around [1,0]) \n",
        "\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/teszth2.jpg\")#, cv2.IMREAD_GRAYSCALE)\n",
        "new_array=cv2.resize(img_array, (128, 128))\n",
        "new_array1=new_array/255 \n",
        "test_np = np.array(new_array1)\n",
        "test_np_float32=test_np.astype('float32')\n",
        "test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "test_tensor=test_tensor0.reshape(3,128,128)\n",
        "\n",
        "predict_image(test_tensor, model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9890, 0.0061]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g89dmgWU7BuR"
      },
      "source": [
        "#----------------Using model resnet18 PRETRAINED on ImageNet to get predictions, the result is bad------\n",
        "\n",
        "\n",
        "#No transfer learning\n",
        "import torchvision.models as models\n",
        "\n",
        "resnet18 = models.resnet18()\n",
        "resnet18\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Bx2c6x_wr2"
      },
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(\"/content/drive/MyDrive/teszth2.jpg\")\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpzVFo5Z8edW"
      },
      "source": [
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    return preds.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l430kVUzEadr"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/942d3a0ac09ec9e5eb3a-238f720ff059c1f82f368259d1ca4ffa5dd8f9f5.zip\" -d \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJM_ybQFBZy"
      },
      "source": [
        "path=\"/content/942d3a0ac09ec9e5eb3a-238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
        "with open(path) as f:\n",
        "  classes = [i.strip() for i in f.readlines()]\n",
        "\n",
        "classes2=[]\n",
        "classes2.append(classes[0][1:])\n",
        "for i in range(1, len(classes)-1):\n",
        "  classes2.append(classes[i])\n",
        "\n",
        "\n",
        "classes3=[]\n",
        "for i in classes2:\n",
        "  classes3.append(i.split(\":\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQKK8bsI8Rhf"
      },
      "source": [
        "#Prediction for image containing just horse  \n",
        "\n",
        "\n",
        "prediction=predict_image(input_tensor, resnet18)\n",
        "\n",
        "for i in classes3:\n",
        "\n",
        "  if i[0]==str(prediction):\n",
        "    print(prediction, i[0],i[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2oBP0jPskem"
      },
      "source": [
        "#--------------------------TRANSFER LEARNING------------------------------\n",
        "\n",
        "import torchvision.models as models\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "    return F2.mean(0)\n",
        "\n",
        "class MultilabelImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, targets = batch \n",
        "        out = self(images)                      \n",
        "        loss = F.binary_cross_entropy(out, targets)      \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch \n",
        "        out = self(images)                           # Generate predictions\n",
        "        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n",
        "        score = F_score(out, targets)\n",
        "        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_scores = [x['val_score'] for x in outputs]\n",
        "        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq7Gl2PAtKkt"
      },
      "source": [
        "class HorseDolphin(MultilabelImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use a pretrained model\n",
        "        self.network = models.resnet34(pretrained=True)\n",
        "        # Replace last layer\n",
        "        num_ftrs = self.network.fc.in_features\n",
        "        self.network.fc = nn.Linear(num_ftrs, 2)\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        return torch.sigmoid(self.network(xb))\n",
        "    \n",
        "    def freeze(self):\n",
        "        # To freeze the residual layers\n",
        "        for param in self.network.parameters():\n",
        "            param.require_grad = False\n",
        "        for param in self.network.fc.parameters():\n",
        "            param.require_grad = True\n",
        "    \n",
        "    def unfreeze(self):\n",
        "        # Unfreeze all layers\n",
        "        for param in self.network.parameters():\n",
        "            param.require_grad = True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBu0uYOXxxMq"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "device\n",
        "\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "model = to_device(HorseDolphin(), device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuiG-35cyFMf"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "    \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in tqdm(train_loader, leave=False):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN6jjEMCKC6r"
      },
      "source": [
        "model.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vm7YC-NOa6F"
      },
      "source": [
        "history = [evaluate(model, val_dl)]\n",
        "history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyrrWmokKEa6"
      },
      "source": [
        "epochs = 5\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam\n",
        "\n",
        "\n",
        "history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n",
        "                         grad_clip=grad_clip, \n",
        "                         weight_decay=weight_decay, \n",
        "                         opt_func=opt_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ej9YkOvKKtp"
      },
      "source": [
        "model.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNTDI_H5KNl5"
      },
      "source": [
        "\n",
        "history += fit_one_cycle(epochs, 0.001, model, train_dl, val_dl, \n",
        "                         grad_clip=grad_clip, \n",
        "                         weight_decay=weight_decay, \n",
        "                         opt_func=opt_func)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvVRRBl6KT6Z"
      },
      "source": [
        "\n",
        "scores = [x['val_score'] for x in history]\n",
        "plt.plot(scores, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.title('F1 score vs. No. of epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jywOg6uXKHlh"
      },
      "source": [
        "\n",
        "train_losses = [x.get('train_loss') for x in history]\n",
        "val_losses = [x['val_loss'] for x in history]\n",
        "plt.plot(train_losses, '-bx')\n",
        "plt.plot(val_losses, '-rx')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.title('Loss vs. No. of epochs');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KM41uKyQGWf"
      },
      "source": [
        "torch.save(model.state_dict(), 'pics.modelparameters')\n",
        "model2=HorseDolphin()\n",
        "model2.load_state_dict(torch.load('pics.modelparameters'))\n",
        "model2.state_dict\n",
        "\n",
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    #_, preds  = torch.max(yb, dim=1)\n",
        "    return yb\n",
        "\n",
        "#Prediction for image containing dolphin and horse (it should be around [1,1])\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/tesztdh.jpg\")#, cv2.IMREAD_GRAYSCALE)\n",
        "new_array=cv2.resize(img_array, (128, 128))\n",
        "new_array1=new_array/255 \n",
        "test_np = np.array(new_array1)\n",
        "test_np_float32=test_np.astype('float32')\n",
        "test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "test_tensor=test_tensor0.reshape(3,128,128)\n",
        "\n",
        "predict_image(test_tensor, model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyuOKHecQl9s"
      },
      "source": [
        "#Prediction for image containing just dolphin (it should be around [0,1])\n",
        "\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/tesztd2.jpg\")#, cv2.IMREAD_GRAYSCALE)\n",
        "new_array=cv2.resize(img_array, (128, 128))\n",
        "new_array1=new_array/255 \n",
        "test_np = np.array(new_array1)\n",
        "test_np_float32=test_np.astype('float32')\n",
        "test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "test_tensor=test_tensor0.reshape(3,128,128)\n",
        "\n",
        "predict_image(test_tensor, model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5XxHzr9QuGb"
      },
      "source": [
        "#Prediction for image containing just horse (it should be around [1,0]) \n",
        "\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/teszth2.jpg\")#, cv2.IMREAD_GRAYSCALE)\n",
        "new_array=cv2.resize(img_array, (128, 128))\n",
        "new_array1=new_array/255 \n",
        "test_np = np.array(new_array1)\n",
        "test_np_float32=test_np.astype('float32')\n",
        "test_tensor0 = torch.from_numpy(test_np_float32)\n",
        "test_tensor=test_tensor0.reshape(3,128,128)\n",
        "\n",
        "predict_image(test_tensor, model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01muFRUmusof"
      },
      "source": [
        "torch.save(model.state_dict(), 'pics.modelparameters')\n",
        "model2=ResNet9(3,2)\n",
        "model2.load_state_dict(torch.load('pics.modelparameters'))\n",
        "model2.state_dict\n",
        "\n",
        "def predict_image(img, model):\n",
        "    #xb = img.unsqueeze(0)\n",
        "    yb = model(img)\n",
        "    #_, preds  = torch.max(yb, dim=1)\n",
        "    return yb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_xWDfaAyHzW"
      },
      "source": [
        "path=\".\"\n",
        "os.chdir(path)\n",
        "os.makedirs(\"/content/tesztek\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrB959Q0xYi2"
      },
      "source": [
        "Archive('/content/HorseDolphinTest.rar').extractall('/content/tesztek/')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pdox70yXA0"
      },
      "source": [
        "teszt_tfms11 = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                         transforms.ToTensor()])\n",
        "\n",
        "ds_teszt=ImageFolder(\"/content/tesztek\", teszt_tfms11)\n",
        "\n",
        "ds_teszt0=[]\n",
        "for i in ds_teszt:\n",
        "  ds_teszt0.append(i[0])\n",
        "\n",
        "ds_teszt1=torch.stack(ds_teszt0)\n",
        "\n",
        "#batch_size=8\n",
        "#ds_teszt1 = DataLoader(ds_teszt0, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "#for batch in ds_teszt1:\n",
        " # print(torch.stack(batch))\n",
        "a=predict_image(ds_teszt1, model2)\n",
        "\n",
        "counterh=0\n",
        "counterd=0\n",
        "counterdh=0\n",
        "for i in a:\n",
        "  if i[0]>0.5 and i[1]<0.5:\n",
        "    counterh +=1\n",
        "  if i[0]<0.5 and i[1]>0.5:\n",
        "    counterd +=1\n",
        "  if i[0]>0.5 and i[1]>0.5: \n",
        "    counterdh+=1\n",
        "\n",
        "print(counterh, counterd, counterdh)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}